{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f443db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c7ebd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review objects loaded. Count = 100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31154</th>\n",
       "      <td>Yummy! And pretty authenic. I'm here from San ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64633</th>\n",
       "      <td>I've been coming here for years, Donna the man...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "31154  Yummy! And pretty authenic. I'm here from San ...      1\n",
       "64633  I've been coming here for years, Donna the man...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27441</th>\n",
       "      <td>Received dead flower on Mother's Day. \\nI know...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8478</th>\n",
       "      <td>This place has the best patio ever! And the be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "27441  Received dead flower on Mother's Day. \\nI know...      0\n",
       "8478   This place has the best patio ever! And the be...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "REVIEWS_LIMIT = 100000 #300000\n",
    "\n",
    "def load_rows(filepath, nrows = None, func = None) -> pd.DataFrame :\n",
    "    with open(filepath) as json_file:\n",
    "        count = 0\n",
    "        objs = []\n",
    "        line = json_file.readline()\n",
    "        while (nrows is None or count < nrows) and line:\n",
    "            count += 1\n",
    "            obj = json.loads(line)\n",
    "            if func != None :\n",
    "                func(obj)\n",
    "            objs.append(obj)\n",
    "            line = json_file.readline()\n",
    "        return pd.DataFrame(objs)\n",
    "    \n",
    "# Aggiunge la classe della recensione\n",
    "def add_sentiment(obj) :\n",
    "    if (obj[\"stars\"] <= 3):\n",
    "        obj[\"label\"] = 0\n",
    "    else:\n",
    "        obj[\"label\"] = 1\n",
    "        \n",
    "reviews = load_rows('input/yelp_academic_dataset_review.json', REVIEWS_LIMIT, add_sentiment)\n",
    "print('Review objects loaded. Count = {}'.format(reviews.shape[0]))\n",
    "\n",
    "reviews['text_length'] = reviews['text'].apply(lambda x:len(x.split()))\n",
    "\n",
    "# 80% train, 20% test\n",
    "reviews_train, reviews_test = train_test_split(reviews, test_size = 0.2)\n",
    "\n",
    "# Solo text, label\n",
    "reviews_train = reviews_train[['text', 'label']]\n",
    "reviews_test = reviews_test[['text', 'label']]\n",
    "display(reviews_train.head(2))\n",
    "display(reviews_test.head(2))\n",
    "\n",
    "#with pd.option_context('display.max_colwidth', None):\n",
    "#  display(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22613f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80000it [01:50, 722.42it/s]\n",
      "20000it [00:13, 1481.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31154</th>\n",
       "      <td>yummy and pretty authenim here from san diego ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64633</th>\n",
       "      <td>i have been coming here for years donna the ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15280</th>\n",
       "      <td>i went last night witriend of mine and did the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36766</th>\n",
       "      <td>took my parents here while they were in town b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73242</th>\n",
       "      <td>came witmall group right at sunday opening tim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "31154  yummy and pretty authenim here from san diego ...      1\n",
       "64633  i have been coming here for years donna the ma...      1\n",
       "15280  i went last night witriend of mine and did the...      0\n",
       "36766  took my parents here while they were in town b...      0\n",
       "73242  came witmall group right at sunday opening tim...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import copy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def contractions(sent):\n",
    "    sent = re.sub(r\"ain't\", \"am not\", sent)\n",
    "    sent = re.sub(r\"aren't\", \"are not\", sent)\n",
    "    sent = re.sub(r\"can't\", \"can not\", sent)\n",
    "    sent = re.sub(r\"can't've\", \"can not have\", sent)\n",
    "    sent = re.sub(r\"'cause\", \"because\", sent)\n",
    "    sent = re.sub(r\"could've\", \"could have\", sent)\n",
    "    sent = re.sub(r\"couldn't\", \"could not\", sent)\n",
    "    sent = re.sub(r\"couldn't've\", \"could not have\", sent)\n",
    "    sent = re.sub(r\"doesn't\", \"does not\", sent)\n",
    "    sent = re.sub(r\"hadn't\", \"had not\", sent)\n",
    "    sent = re.sub(r\"hadn't've\", \"had not have\", sent)\n",
    "    sent = re.sub(r\"hasn't\", \"has not\", sent)\n",
    "    sent = re.sub(r\"haven't\", \"have not\", sent)\n",
    "    sent = re.sub(r\"he'd\", \"he had\", sent)\n",
    "    sent = re.sub(r\"he'd've\", \"he would have\", sent)\n",
    "    sent = re.sub(r\"he'll\", \"he will\", sent)\n",
    "    sent = re.sub(r\"he'll've\", \"he will have\", sent)\n",
    "    sent = re.sub(r\"he's\", \"he has\", sent)\n",
    "    sent = re.sub(r\"how'd\", \"how did\", sent)\n",
    "    sent = re.sub(r\"how'd'y\", \"how do you\", sent)\n",
    "    sent = re.sub(r\"how'll\", \"how will\", sent)\n",
    "    sent = re.sub(r\"how's\", \"how has\", sent)\n",
    "    sent = re.sub(r\"i'd\", \"i had\", sent)\n",
    "    sent = re.sub(r\"i'd've\", \"i would have\", sent)\n",
    "    sent = re.sub(r\"i'll\", \"i shall\", sent)\n",
    "    sent = re.sub(r\"i'll've\", \"i shall have\", sent)\n",
    "    sent = re.sub(r\"i'm\", \"i am\", sent)\n",
    "    sent = re.sub(r\"i've\", \"i have\", sent)\n",
    "    sent = re.sub(r\"isn't\", \"is not\", sent)\n",
    "    sent = re.sub(r\"it'd\", \"it had\", sent)\n",
    "    sent = re.sub(r\"it'd've\", \"it would have\", sent)\n",
    "    sent = re.sub(r\"it'll\", \"it shall\", sent)\n",
    "    sent = re.sub(r\"it'll've\", \"it shall have\", sent)\n",
    "    sent = re.sub(r\"it's\", \"it is\", sent)\n",
    "    sent = re.sub(r\"let's\", \"let us\", sent)\n",
    "    sent = re.sub(r\"ma'am\", \"madam\", sent)\n",
    "    sent = re.sub(r\"mayn't\", \"may not\", sent)\n",
    "    sent = re.sub(r\"might've\", \"might have\", sent)\n",
    "    sent = re.sub(r\"mightn't\", \"might not\", sent)\n",
    "    sent = re.sub(r\"mightn't've\", \"might not have\", sent)\n",
    "    sent = re.sub(r\"must've\", \"must have\", sent)\n",
    "    sent = re.sub(r\"mustn't\", \"must not\", sent)\n",
    "    sent = re.sub(r\"mustn't've\", \"must not have\", sent)\n",
    "    sent = re.sub(r\"needn't\", \"need not\", sent)\n",
    "    sent = re.sub(r\"needn't've\", \"need not have\", sent)\n",
    "    sent = re.sub(r\"o'clock\", \"of the clock\", sent)\n",
    "    sent = re.sub(r\"oughtn't\", \"ought not\", sent)\n",
    "    sent = re.sub(r\"oughtn't've\", \"ought not have\", sent)\n",
    "    sent = re.sub(r\"shan't\", \"shall not\", sent)\n",
    "    sent = re.sub(r\"sha'n't\", \"shall not\", sent)\n",
    "    sent = re.sub(r\"shan't've\", \"shall not have\", sent)\n",
    "    sent = re.sub(r\"she'd\", \"she had\", sent)\n",
    "    sent = re.sub(r\"she'd've\", \"she would have\", sent)\n",
    "    sent = re.sub(r\"she'll\", \"she shall\", sent)\n",
    "    sent = re.sub(r\"she'll've\", \"she shall have\", sent)\n",
    "    sent = re.sub(r\"she's\", \"she has\", sent)\n",
    "    sent = re.sub(r\"should've\", \"should have\", sent)\n",
    "    sent = re.sub(r\"shouldn't\", \"should not\", sent)\n",
    "    sent = re.sub(r\"shouldn't've\", \"should not have\", sent)\n",
    "    sent = re.sub(r\"so've\", \"so have\", sent)\n",
    "    sent = re.sub(r\"so's\", \"so as\", sent)\n",
    "    sent = re.sub(r\"that'd\", \"that would\", sent)\n",
    "    sent = re.sub(r\"that'd've\", \"that would have\", sent)\n",
    "    sent = re.sub(r\"that's\", \"that has\", sent)\n",
    "    sent = re.sub(r\"there'd\", \"there had\", sent)\n",
    "    sent = re.sub(r\"there'd've\", \"there would have\", sent)\n",
    "    sent = re.sub(r\"there's\", \"there has\", sent)\n",
    "    sent = re.sub(r\"they'd\", \"they had\", sent)\n",
    "    sent = re.sub(r\"they'd've\", \"they would have\", sent)\n",
    "    sent = re.sub(r\"they'll\", \"they shall\", sent)\n",
    "    sent = re.sub(r\"they'll've\", \"they shall have\", sent)\n",
    "    sent = re.sub(r\"they're\", \"they are\", sent)\n",
    "    sent = re.sub(r\"they've\", \"they have\", sent)\n",
    "    sent = re.sub(r\"to've\", \"to have\", sent)\n",
    "    sent = re.sub(r\"wasn't\", \"was not\", sent)\n",
    "    sent = re.sub(r\"we'd\", \"we had\", sent)\n",
    "    sent = re.sub(r\"we'd've\", \"we would have\", sent)\n",
    "    sent = re.sub(r\"we'll\", \"we will\", sent)\n",
    "    sent = re.sub(r\"we'll've\", \"we will have\", sent)\n",
    "    sent = re.sub(r\"we're\", \"we are\", sent)\n",
    "    sent = re.sub(r\"we've\", \"we have\", sent)\n",
    "    sent = re.sub(r\"weren't\", \"were not\", sent)\n",
    "    sent = re.sub(r\"what'll\", \"what shall\", sent)\n",
    "    sent = re.sub(r\"what'll've\", \"what shall have\", sent)\n",
    "    sent = re.sub(r\"what're\", \"what are\", sent)\n",
    "    sent = re.sub(r\"what's\", \"what has\", sent)\n",
    "    sent = re.sub(r\"what've\", \"what have\", sent)\n",
    "    sent = re.sub(r\"when's\", \"when has\", sent)\n",
    "    sent = re.sub(r\"when've\", \"when have\", sent)\n",
    "    sent = re.sub(r\"where'd\", \"where did\", sent)\n",
    "    sent = re.sub(r\"where's\", \"where has\", sent)\n",
    "    sent = re.sub(r\"where've\", \"where have\", sent)\n",
    "    sent = re.sub(r\"who'll\", \"who shall\", sent)\n",
    "    sent = re.sub(r\"who'll've\", \"who shall have\", sent)\n",
    "    sent = re.sub(r\"who's\", \"who has\", sent)\n",
    "    sent = re.sub(r\"who've\", \"who have\", sent)\n",
    "    sent = re.sub(r\"why's\", \"why has\", sent)\n",
    "    sent = re.sub(r\"why've\", \"why have\", sent)\n",
    "    sent = re.sub(r\"will've\", \"will have\", sent)\n",
    "    sent = re.sub(r\"won't\", \"will not\", sent)\n",
    "    sent = re.sub(r\"won't've\", \"will not have\", sent)\n",
    "    sent = re.sub(r\"would've\", \"would have\", sent)\n",
    "    sent = re.sub(r\"wouldn't\", \"would not\", sent)\n",
    "    sent = re.sub(r\"wouldn't've\", \"would not have\", sent)\n",
    "    sent = re.sub(r\"y'all\", \"you all\", sent)\n",
    "    sent = re.sub(r\"y'all'd\", \"you all would\", sent)\n",
    "    sent = re.sub(r\"y'all'd've\", \"you all would have\", sent)\n",
    "    sent = re.sub(r\"y'all're\", \"you all are\", sent)\n",
    "    sent = re.sub(r\"y'all've\", \"you all have\", sent)\n",
    "    sent = re.sub(r\"you'd\", \"you had\", sent)\n",
    "    sent = re.sub(r\"you'd've\", \"you would have\", sent)\n",
    "    sent = re.sub(r\"you'll\", \"you shall\", sent)\n",
    "    sent = re.sub(r\"you'll've\", \"you shall have\", sent)\n",
    "    sent = re.sub(r\"how's\", \"how has\", sent)\n",
    "    sent = re.sub(r\"you're\", \"you are\", sent)\n",
    "    sent = re.sub(r\"you've\", \"you have\", sent)\n",
    "    sent = re.sub(r\"didn't\", \"did not\", sent)\n",
    "    sent = re.sub(r\"don't\", \"do not\", sent)\n",
    "    sent = re.sub(r\"'\",\"\",sent)\n",
    "    sent = re.sub(r\". . .\",\"\",sent)\n",
    "    return(sent)\n",
    "\n",
    "## Function for removing unwanted text\n",
    "def processing(data_1):\n",
    " \n",
    "    for index, row in tqdm(data_1.iterrows()):\n",
    "        stri = \"\"\n",
    "## Code to remove digit with word pattern\n",
    "        cle = re.sub(r'([\\d]+[a-zA-Z]+)|([a-zA-Z]+[\\d]+)', \"\", row[\"text\"])\n",
    "## Code to remove only digit patter\n",
    "        cle = re.sub(r\"(^|\\s)(\\-?\\d+(?:\\.\\d)*|\\d+|[\\d]+[A-Za-z]+)\",\" \", cle.lower())\n",
    "## Code to remove every symbols except characters\n",
    "        cle = re.sub('[^A-Za-z\\']+', \" \", cle)\n",
    "## Code for concatinating strings\n",
    "        stri = stri + cle\n",
    "## Code for calling contraction function\n",
    "        stri = contractions(stri)\n",
    "        data_1[\"text\"][index] = stri\n",
    "    return(data_1)\n",
    "\n",
    "## Function for stopwords removal and lemitizing the word\n",
    "def lema_stopw(data_l):\n",
    "    var2 = copy.deepcopy(data_l)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english')) - set(['no', 'not'])\n",
    "    for index, row in tqdm(var2.iterrows()):\n",
    "        sent = ''\n",
    "        for e in row[\"text\"].split():\n",
    "            if e not in stop_words:\n",
    "                e = lemmatizer.lemmatize(e, pos =\"a\")\n",
    "                sent = ' '.join([sent,e])\n",
    "        var2[\"text\"][index] = sent\n",
    "    return(var2)\n",
    "\n",
    "reviews_train = processing(reviews_train)\n",
    "reviews_test = processing(reviews_test)\n",
    "reviews_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8716ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text vectorization\n",
    "# Bigram Counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from joblib import dump, load # used for saving and loading sklearn objects\n",
    "\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "bigram_vectorizer.fit(reviews_train['text'].values)\n",
    "\n",
    "\n",
    "X_train_bigram = bigram_vectorizer.transform(reviews_train['text'].values)\n",
    "X_test_bigram = bigram_vectorizer.transform(reviews_test['text'].values)\n",
    "\n",
    "# Bigram Tf-Idf\n",
    "\n",
    "bigram_tf_idf_transformer = TfidfTransformer()\n",
    "bigram_tf_idf_transformer.fit(X_train_bigram)\n",
    "\n",
    "X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n",
    "y_train = reviews_train['label'].values\n",
    "\n",
    "X_test_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_test_bigram)\n",
    "y_test = reviews_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c63a5e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "--- Ended in 71.47292774120966 minutes ---\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "--- Ended in 71.19827625751495 minutes ---\n",
      "\n",
      "KNN Bigram\n",
      "Best params: {'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best score: 0.8014944130871037\n",
      "\n",
      "KNN Bigram Tf-Idf\n",
      "Best params: {'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best score: 0.6460750009333186\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import uniform\n",
    "import time\n",
    "\n",
    "# loss, learning rate, initial learning rate, penalty and alpha\n",
    "\n",
    "params = {\n",
    "    \"n_neighbors\" : [3, 5, 10],\n",
    "    \"weights\" : [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "grid_search_knn_bigram = GridSearchCV(\n",
    "    estimator= KNeighborsClassifier(n_jobs=-1),\n",
    "    cv=5,\n",
    "    param_grid = params,\n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "grid_search_knn_tf_idf = GridSearchCV(\n",
    "    estimator= KNeighborsClassifier(n_jobs=-1),\n",
    "    cv=5,\n",
    "    param_grid = params,\n",
    "    verbose=1,\n",
    "    scoring=\"f1\",\n",
    "    return_train_score = True\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search_knn_bigram.fit(X_train_bigram, y_train)\n",
    "print(\"--- Ended in %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search_knn_tf_idf.fit(X_train_bigram_tf_idf, y_train)\n",
    "print(\"--- Ended in %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "print(f'\\nKNN Bigram')\n",
    "print(f'Best params: {grid_search_knn_bigram.best_params_}')\n",
    "print(f'Best score: {grid_search_knn_bigram.best_score_}')\n",
    "\n",
    "print(f'\\nKNN Bigram Tf-Idf')\n",
    "print(f'Best params: {grid_search_knn_tf_idf.best_params_}')\n",
    "print(f'Best score: {grid_search_knn_tf_idf.best_score_}')\n",
    "\n",
    "knn_classifier_bigram = grid_search_knn_bigram.best_estimator_\n",
    "knn_classifier_tf_idf = grid_search_knn_tf_idf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7114c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluate(y_test,y_pred):\n",
    "    print(\"Precision Score of the model:\", precision_score(y_test,y_pred)*100)\n",
    "    print(\"Recall Score of the model:\", recall_score(y_test,y_pred)*100)\n",
    "    print(\"Acuracy score of the model:\",accuracy_score(y_test,y_pred)*100)\n",
    "    print(\"F1 score of the model:\",f1_score(y_test,y_pred)*100)\n",
    "    \n",
    "def set_labels(cf_matrix):\n",
    "    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb5e6d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Bigram\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.33      0.44      6435\n",
      "           1       0.74      0.91      0.82     13565\n",
      "\n",
      "    accuracy                           0.72     20000\n",
      "   macro avg       0.69      0.62      0.63     20000\n",
      "weighted avg       0.71      0.72      0.69     20000\n",
      "\n",
      "Precision Score of the model: 74.18887950790013\n",
      "Recall Score of the model: 90.68927386656837\n",
      "Acuracy score of the model: 72.285\n",
      "F1 score of the model: 81.6134275383952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import seaborn as sn\n",
    "\n",
    "print(\"KNN Bigram\")\n",
    "y_pred = knn_classifier_bigram.predict(X_test_bigram)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "evaluate(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f95c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
